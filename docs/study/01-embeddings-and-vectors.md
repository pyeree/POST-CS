# 01. 임베딩과 벡터

> **핵심 질문:** 컴퓨터가 텍스트의 "의미"를 어떻게 이해하는가?

---

## 1. 임베딩이란?

**임베딩(Embedding)** = 텍스트를 숫자 배열(벡터)로 변환하는 것

```
"Python GIL은 멀티스레딩을 제한한다"
        ↓ 임베딩 모델
[0.12, -0.45, 0.78, 0.33, ..., -0.21]    ← 768개 또는 1536개의 숫자
```

왜 이게 필요한가?
- 컴퓨터는 문자열을 "이해"하지 못한다 — 숫자만 연산할 수 있다
- 텍스트를 숫자로 바꿔야 "비슷한 의미"를 수학적으로 계산할 수 있다

### 핵심 직관

**의미가 비슷한 텍스트는 비슷한 벡터를 가진다.**

```
"Python GIL은 멀티스레딩을 제한한다"  → [0.12, -0.45, 0.78, ...]
"파이썬의 전역 인터프리터 락"         → [0.13, -0.44, 0.77, ...]  ← 아주 가까움!
"오늘 점심 뭐 먹지"                  → [0.89, 0.22, -0.56, ...]  ← 완전 다름
```

---

## 2. 벡터 공간

### 벡터란?

수학적으로 벡터는 **방향과 크기를 가진 점**이다. 2차원이면 이해하기 쉽다:

```
         ^  y축
         |
    "GIL" ●          ← (0.12, 0.78)
         |
         |        ● "점심"  ← (0.89, -0.56)
    -----+----------->  x축
         |
```

실제 임베딩은 768차원 또는 1536차원이라 시각화할 수 없지만, 원리는 같다.
의미가 비슷한 텍스트끼리 벡터 공간에서 가까이 모여있다.

### 차원(Dimension)

| 모델 | 차원 수 | 의미 |
|------|---------|------|
| OpenAI text-embedding-3-small | 1536 | 1536개의 숫자로 의미를 표현 |
| Cohere embed-v4 | 1024 | 1024개의 숫자 |
| BGE-M3 (오픈소스) | 1024 | 셀프호스팅 가능 |

차원이 높을수록 더 세밀한 의미 차이를 표현할 수 있지만, 저장 공간과 연산 비용이 증가한다.

---

## 3. 유사도 측정

두 벡터가 얼마나 비슷한지 측정하는 방법.

### 코사인 유사도 (Cosine Similarity) — 가장 많이 쓰임

**두 벡터 사이의 각도**를 측정한다. 방향이 같으면 1, 반대면 -1, 직각이면 0.

```
         ^
    A ● /
      | /  θ = 작은 각도 → 유사도 높음 (≈ 0.95)
      | /
      |/
      +-------> B ●    θ = 큰 각도 → 유사도 낮음 (≈ 0.2)
```

**수식** (외울 필요 없음, 직관만 이해):
```
cosine_similarity(A, B) = (A · B) / (|A| × |B|)
```
- A · B = 두 벡터의 내적 (원소별 곱의 합)
- |A|, |B| = 각 벡터의 크기

### Python 코드로 보면

```python
import numpy as np

# 두 문장의 임베딩 벡터 (실제로는 임베딩 모델이 생성)
vec_a = np.array([0.12, -0.45, 0.78])   # "Python GIL"
vec_b = np.array([0.13, -0.44, 0.77])   # "파이썬 전역 인터프리터 락"
vec_c = np.array([0.89, 0.22, -0.56])   # "오늘 점심 뭐 먹지"

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print(cosine_similarity(vec_a, vec_b))  # ≈ 0.999 (거의 같은 의미)
print(cosine_similarity(vec_a, vec_c))  # ≈ -0.34 (완전 다른 의미)
```

### 다른 유사도 측정 방법

| 방법 | 특징 | 언제 쓰나 |
|------|------|----------|
| **코사인 유사도** | 방향(의미) 비교. 문서 길이에 무관 | 텍스트 검색 (가장 일반적) |
| **유클리드 거리** | 절대적 거리. 크기 차이에 민감 | 이미지, 수치 데이터 |
| **내적 (Dot Product)** | 방향 + 크기. 빠른 연산 | 대규모 검색 (근사 최근접 이웃) |

대부분의 텍스트 RAG에서는 **코사인 유사도**를 쓴다.

---

## 4. 임베딩 모델

텍스트를 벡터로 바꿔주는 모델. LLM과는 다른 모델이다.

### LLM vs 임베딩 모델

| | LLM (생성 모델) | 임베딩 모델 |
|---|---|---|
| **하는 일** | 텍스트 생성 (다음 토큰 예측) | 텍스트 → 벡터 변환 |
| **출력** | 문장/문단 | 숫자 배열 [0.12, -0.45, ...] |
| **크기** | 수십~수백B 파라미터 | 수백M 파라미터 (훨씬 작음) |
| **비용** | 높음 | 낮음 |
| **예시** | GPT-4, Claude, Llama | text-embedding-3, BGE-M3 |

### 2026년 주요 임베딩 모델

**클라우드 API:**
- `text-embedding-3-large` (OpenAI) — 안정적, 생태계 넓음
- `embed-v4` (Cohere) — 128K 토큰 컨텍스트, 다국어 강점

**오픈소스 (셀프호스팅):**
- `BGE-M3` — Dense + Sparse + Multi-vector 통합, 한국어도 지원
- `multilingual-e5-large` — 다국어 특화

### Post-CS에서의 선택

온프레미스 LLM 서버가 있으니:
- 서버에 `BGE-M3` 같은 오픈소스 임베딩 모델을 올리는 게 자연스러움
- 또는 LLM 서버의 임베딩 API가 있다면 그걸 활용

---

## 5. 임베딩이 Post-CS에서 어떻게 쓰이는가

```
[기존 발행된 블로그 글]
    "Python GIL 완전 정복"
    "Docker 네트워크 비교"
    "REST API 설계 원칙"
        │
        ▼ 임베딩 모델
        │
[벡터로 변환되어 ChromaDB에 저장]
    [0.12, -0.45, 0.78, ...]  → "Python GIL"
    [0.56, 0.11, -0.33, ...]  → "Docker 네트워크"
    [0.34, 0.67, 0.12, ...]   → "REST API"

[새 글 생성 시]
    "Python 동시성 프로그래밍" 주제로 글을 쓰려 함
        │
        ▼ 임베딩
    [0.11, -0.43, 0.76, ...]
        │
        ▼ 코사인 유사도 검색
        │
    가장 비슷한 글 = "Python GIL 완전 정복" (유사도: 0.92)
        │
        ▼
    "이전에 GIL 글을 썼으니, 중복 설명을 빼고 링크를 걸자"
    → 이 정보를 LLM 프롬프트에 넣어줌 (= RAG)
```

---

## 6. 직접 해보기

### 실습 1: 코사인 유사도 직접 계산

```python
# numpy만으로 임베딩 유사도를 체감해보기
import numpy as np

# 가짜 임베딩 (실제로는 임베딩 모델이 생성)
sentences = {
    "Python의 GIL": np.random.randn(10),
    "파이썬 전역 인터프리터 락": None,  # 아래에서 비슷하게 만듦
    "맛있는 치킨 레시피": np.random.randn(10),
}

# "Python의 GIL"과 비슷한 벡터를 만들어보자
sentences["파이썬 전역 인터프리터 락"] = sentences["Python의 GIL"] + np.random.randn(10) * 0.1

# 유사도 계산
def cosine_sim(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

base = sentences["Python의 GIL"]
for name, vec in sentences.items():
    sim = cosine_sim(base, vec)
    print(f"  '{name}' vs 'Python의 GIL' → 유사도: {sim:.4f}")
```

### 실습 2: 실제 임베딩 모델 사용 (선택)

```python
# pip install sentence-transformers
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('BAAI/bge-m3')

sentences = [
    "Python GIL은 멀티스레딩을 제한한다",
    "파이썬의 전역 인터프리터 락 문제",
    "오늘 점심으로 치킨을 먹었다",
]

embeddings = model.encode(sentences)

# 유사도 확인
from sklearn.metrics.pairwise import cosine_similarity
sims = cosine_similarity(embeddings)
print(sims)
# [[1.0,  0.85, 0.05],    ← GIL vs 전역 인터프리터 락 = 높음
#  [0.85, 1.0,  0.03],
#  [0.05, 0.03, 1.0 ]]    ← 치킨 vs 나머지 = 낮음
```

---

## 핵심 정리

| 개념 | 한 줄 요약 |
|------|-----------|
| 임베딩 | 텍스트를 숫자 배열(벡터)로 변환하는 것 |
| 벡터 | 의미를 표현하는 숫자 배열. 비슷한 의미 = 비슷한 벡터 |
| 코사인 유사도 | 두 벡터의 방향이 얼마나 같은지 (-1 ~ 1) |
| 임베딩 모델 | 텍스트 → 벡터를 만들어주는 (LLM보다 가벼운) 모델 |

**다음 단계:** 이 벡터들을 효율적으로 저장하고 검색하는 방법 → `02-vector-databases.md`
